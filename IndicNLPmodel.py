# -*- coding: utf-8 -*-
"""Info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Vbnpt1tvcQWOB4ZyY7_VjIXpeqGMQJ_
"""

pip install torch transformers scikit-learn sentencepiece tokenizers

pip install indicnlp



pip install onnx

pip install onnx-tf

pip install tensorflow

pip install numpy

pip install requests

import os
import torch
import torch.nn as nn
import torch.onnx
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
from transformers import AutoConfig, AutoModelForSequenceClassification, get_scheduler, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tqdm import tqdm
from tokenizers import SentencePieceBPETokenizer, models, pre_tokenizers, decoders, processors
from torch.utils.tensorboard import SummaryWriter
from onnx_tf.backend import prepare
import onnx
import tensorflow as tf
import numpy as np
import requests

# Define the dataset
class IntentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer.encode_batch([self.texts[idx]])[0]
        inputs = {
            'input_ids': torch.tensor(encoding.ids),
            'attention_mask': torch.tensor(encoding.attention_mask),
            'token_type_ids': torch.tensor(encoding.type_ids),
        }
        label = torch.tensor(self.labels[idx])
        return inputs, label

# Example dataset
data = {
    "texts": [
        "Can you help me find a nearby restaurant?",
        "Where is nearest hospital?",
        "I need information about government schemes.",
        "Scholarship schemes by Government of Punjab",
    ],
    "labels": [0, 0, 1,1],
}

# Split the dataset
texts_train, texts_val, labels_train, labels_val = train_test_split(data["texts"], data["labels"], test_size=0.15, random_state=42)

# Train SentencePiece tokenizer
tokenizer_path = "indic_bert_sentencepiece"
texts_train_string = "\n".join(texts_train)

tokenizer = SentencePieceBPETokenizer()
tokenizer.model = models.BPE()
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
tokenizer.decoder = decoders.ByteLevel()
tokenizer.post_processor = processors.TemplateProcessing(
    single="<unk>",
    special_tokens=[
        ("<pad>", 0),
        ("<cls>", 1),
        ("<sep>", 2),
        ("<mask>", 3),
        ("<unk>", 4),
    ],
)
tokenizer.train_from_iterator([texts_train_string], vocab_size=30000, min_frequency=2, special_tokens=["<pad>", "<cls>", "<sep>", "<mask>", "<unk>"])

# Save the trained tokenizer
os.makedirs(tokenizer_path, exist_ok=True)
tokenizer.save_model(tokenizer_path)

# Load fast tokenizer
vocab_filename = f"{tokenizer_path}/vocab.json"
merges_filename = f"{tokenizer_path}/merges.txt"
tokenizer = SentencePieceBPETokenizer(vocab_filename, merges_filename)

# Create datasets
train_dataset = IntentDataset(texts_train, labels_train, tokenizer)
val_dataset = IntentDataset(texts_val, labels_val, tokenizer)

# Define a collate function to handle padding
def collate_fn(batch):
    inputs, labels = zip(*batch)

    # Pad sequences to the maximum length in the batch
    padded_inputs = {
        'input_ids': pad_sequence([item['input_ids'] for item in inputs], batch_first=True),
        'attention_mask': pad_sequence([item['attention_mask'] for item in inputs], batch_first=True),
        'token_type_ids': pad_sequence([item['token_type_ids'] for item in inputs], batch_first=True),
    }

    padded_labels = torch.stack(labels)

    return padded_inputs, padded_labels

# Creating data loaders using the collate fn
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

# Load the configuration
config = AutoConfig.from_pretrained("ai4bharat/indic-bert", num_labels=3)

# Load the PyTorch model
pytorch_model = AutoModelForSequenceClassification.from_pretrained("ai4bharat/indic-bert")

# Set a maximum token length based on your requirements
max_token_length = 128

# Generate a sample input for a sequence of tokens
input_text = "Can you modify this code for Indic BERT?"

# Tokenize the input text
tokens = tokenizer.encode(input_text)

# Truncate or pad the tokens to the desired maximum length
if len(tokens.ids) > max_token_length:
    tokens.ids = tokens.ids[:max_token_length]
    tokens.pieces = tokens.pieces[:max_token_length]
    tokens.tokens = tokens.tokens[:max_token_length]
    tokens.type_ids = tokens.type_ids[:max_token_length]

# Convert tokens to input tensor
input_ids = torch.tensor(tokens.ids).unsqueeze(0)

# Create attention mask
attention_mask = torch.ones_like(input_ids)

# Create token type IDs (for models that use them)
token_type_ids = torch.zeros_like(input_ids)

# Run model inference
with torch.no_grad():
    y = pytorch_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

# Export to ONNX
onnx_model_path = 'indic_bert_model.onnx'
torch.onnx.export(
    pytorch_model,
    (input_ids, attention_mask, token_type_ids),  # Pass the input tensors as a tuple
    onnx_model_path,
    verbose=False,
    input_names=['input_ids', 'attention_mask', 'token_type_ids'],
    output_names=['output'],
    opset_version=12
)

# Convert PyTorch model to TensorFlow SavedModel
onnx_model = onnx.load(onnx_model_path)
tf_rep = prepare(onnx_model)
tf_rep.export_graph('tf_saved_model_dir')

# Convert TensorFlow SavedModel to TensorFlow Lite
saved_model_dir = 'tf_saved_model_dir'
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)

# Define optimizer and loss function for PyTorch model
optimizer = torch.optim.AdamW(pytorch_model.parameters(), lr=1e-5)
criterion = torch.nn.CrossEntropyLoss()

# Training loop
num_epochs = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pytorch_model.to(device)

for epoch in range(num_epochs):
    pytorch_model.train()
    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs}"):
        inputs = {key: value.to(device) for key, value in inputs.items() if key != 'labels'}
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = pytorch_model(**inputs, labels=labels)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()

    # Validation
    pytorch_model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc=f"Validation - Epoch {epoch + 1}/{num_epochs}"):
            inputs = {key: value.to(device) for key, value in inputs.items() if key != 'labels'}
            labels = labels.to(device)
            outputs = pytorch_model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate validation accuracy
    val_accuracy = accuracy_score(all_labels, all_preds)
    #print(f"Validation Accuracy - Epoch {epoch + 1}: {val_accuracy:.4f}")

# Inference with PyTorch model
input_text = input("Ask me anything...")

# Preprocess and tokenize the input
max_length = 64
input_text = "[CLS] " + input_text + " [SEP]"  # Add [CLS] and [SEP] tokens
encoding = tokenizer.encode(input_text)

# Ensure the input does not exceed the model's maximum input length
input_ids = encoding.ids[:max_length]
attention_mask = encoding.attention_mask[:max_length]

# Convert to PyTorch tensors
input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension
attention_mask = torch.tensor(attention_mask).unsqueeze(0)  # Add batch dimension

# Use the encoded input with the PyTorch model
with torch.no_grad():
    pytorch_model.eval()
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    outputs = pytorch_model(input_ids=input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

responses = {
    0: "Here are some nearby places.",
    #2: "Here are the available services.",
    1: "I can provide information about government schemes.",
}

# Get the response based on the predicted intent class
response = responses.get(predicted_class, "Unknown intent.")

'''def integ(predicted_class,response):
  return predicted_class, response'''
# Print the result
print(f"Predicted intent class: {predicted_class}")
print(response)

if predicted_class == 0:  # Nearby places intent
    # Extract important tokens (replace with your method)
    important_tokens = encoding.ids[:4]

    # Construct a query using the important tokens
    query = tokenizer.decode(important_tokens, skip_special_tokens=True)

    # Make a request to a places API
    places_api_url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'
    params = {
        'location': '12.3356607,76.6196155',  # Replace with the user's location coordinates
        'radius': 5000,  # Search radius in meters
        'key': 'AIzaSyCcoljrzOM4U0IdQe3GxGt6DonG-djcz2k',  # Replace with your actual API key
        'keyword': query,  # Use the extracted query
    }
    response = requests.get(places_api_url, params=params)
    places_data = response.json()

    for place in places_data.get('results', []):
        name = place.get('name', 'Unknown Place')
        location = place.get('geometry', {}).get('location', {})
        lat, lng = location.get('lat', ''), location.get('lng', '')
        print(f"Place: {name}, Location: {lat}, {lng}")
if predicted_class == 1:
    ch = input('Which Government (State/Central)? ')
    if ch == "State":
        print('Anna Bhagya is a flagship food security program in Karnataka, providing subsidized food grains to eligible families.')
        print('The scheme aims to ensure food availability and accessibility to all, particularly those belonging to economically disadvantaged backgrounds.\n')
        print('Krishi Bhagya is an agricultural irrigation scheme that promotes efficient water usage in farming.')
        print('Through the distribution of drip and sprinkler irrigation systems, the initiative assists farmers in optimizing water resources.\n')
        print('Mukhyamantri Santwana Harish Yojana, a health insurance scheme provides financial assistance to accident victims for immediate medical treatment.\n')
        print('Indira Canteen is a subsidized meal program that aims to provide nutritious and affordable meals to urban residents, particularly daily-wage laborers and the economically vulnerable.')
        print('The Karnataka Bhagya Laxmi Scheme focuses on the empowerment of women by providing financial assistance to economically backward families during childbirth.\n')
    elif ch==1:
        print('Launched in 2014, PMJDY is a financial inclusion initiative aimed at providing affordable access to banking services for all.')
        print('It focuses on opening bank accounts, providing insurance, and facilitating credit for the unbanked population, contributing to financial empowerment.\n')
        print('Initiated in 2014, Swachh Bharat Abhiyan is a nationwide cleanliness campaign with the goal of achieving an open-defecation-free India.\n')
        print('PM-JAY covers hospitalization expenses, promoting accessibility to quality healthcare services and addressing the financial burden of medical treatments.\n')
        print('Enacted in 2005, MGNREGA guarantees 100 days of wage employment to rural households, contributing to economic security.\n')
        print('Introduced in 2016, PMFBY is a crop insurance scheme aiming to provide financial support to farmers in the event of crop failure.')

